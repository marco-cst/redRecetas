<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
from __future__ import annotations

=======
>>>>>>> feature/Ingredientes
=======
from __future__ import annotations

>>>>>>> feature/Cuenta
=======
from __future__ import annotations

>>>>>>> feature/Categoria
import importlib
from codecs import IncrementalDecoder
from collections import Counter
from functools import lru_cache
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
from typing import Counter as TypeCounter
=======
from typing import Counter as TypeCounter, Dict, List, Optional, Tuple
>>>>>>> feature/Ingredientes
=======
from typing import Counter as TypeCounter
>>>>>>> feature/Cuenta
=======
from typing import Counter as TypeCounter
>>>>>>> feature/Categoria

from .constant import (
    FREQUENCIES,
    KO_NAMES,
    LANGUAGE_SUPPORTED_COUNT,
    TOO_SMALL_SEQUENCE,
    ZH_NAMES,
)
from .md import is_suspiciously_successive_range
from .models import CoherenceMatches
from .utils import (
    is_accentuated,
    is_latin,
    is_multi_byte_encoding,
    is_unicode_range_secondary,
    unicode_range,
)


<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
def encoding_unicode_range(iana_name: str) -> list[str]:
=======
def encoding_unicode_range(iana_name: str) -> List[str]:
>>>>>>> feature/Ingredientes
=======
def encoding_unicode_range(iana_name: str) -> list[str]:
>>>>>>> feature/Cuenta
=======
def encoding_unicode_range(iana_name: str) -> list[str]:
>>>>>>> feature/Categoria
    """
    Return associated unicode ranges in a single byte code page.
    """
    if is_multi_byte_encoding(iana_name):
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> feature/Cuenta
=======
>>>>>>> feature/Categoria
        raise OSError("Function not supported on multi-byte code page")

    decoder = importlib.import_module(f"encodings.{iana_name}").IncrementalDecoder

    p: IncrementalDecoder = decoder(errors="ignore")
    seen_ranges: dict[str, int] = {}
<<<<<<< HEAD
<<<<<<< HEAD
=======
        raise IOError("Function not supported on multi-byte code page")

    decoder = importlib.import_module(
        "encodings.{}".format(iana_name)
    ).IncrementalDecoder

    p: IncrementalDecoder = decoder(errors="ignore")
    seen_ranges: Dict[str, int] = {}
>>>>>>> feature/Ingredientes
=======
>>>>>>> feature/Cuenta
=======
>>>>>>> feature/Categoria
    character_count: int = 0

    for i in range(0x40, 0xFF):
        chunk: str = p.decode(bytes([i]))

        if chunk:
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
            character_range: str | None = unicode_range(chunk)
=======
            character_range: Optional[str] = unicode_range(chunk)
>>>>>>> feature/Ingredientes
=======
            character_range: str | None = unicode_range(chunk)
>>>>>>> feature/Cuenta
=======
            character_range: str | None = unicode_range(chunk)
>>>>>>> feature/Categoria

            if character_range is None:
                continue

            if is_unicode_range_secondary(character_range) is False:
                if character_range not in seen_ranges:
                    seen_ranges[character_range] = 0
                seen_ranges[character_range] += 1
                character_count += 1

    return sorted(
        [
            character_range
            for character_range in seen_ranges
            if seen_ranges[character_range] / character_count >= 0.15
        ]
    )


<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> feature/Cuenta
=======
>>>>>>> feature/Categoria
def unicode_range_languages(primary_range: str) -> list[str]:
    """
    Return inferred languages used with a unicode range.
    """
    languages: list[str] = []
<<<<<<< HEAD
<<<<<<< HEAD
=======
def unicode_range_languages(primary_range: str) -> List[str]:
    """
    Return inferred languages used with a unicode range.
    """
    languages: List[str] = []
>>>>>>> feature/Ingredientes
=======
>>>>>>> feature/Cuenta
=======
>>>>>>> feature/Categoria

    for language, characters in FREQUENCIES.items():
        for character in characters:
            if unicode_range(character) == primary_range:
                languages.append(language)
                break

    return languages


@lru_cache()
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
def encoding_languages(iana_name: str) -> list[str]:
=======
def encoding_languages(iana_name: str) -> List[str]:
>>>>>>> feature/Ingredientes
=======
def encoding_languages(iana_name: str) -> list[str]:
>>>>>>> feature/Cuenta
=======
def encoding_languages(iana_name: str) -> list[str]:
>>>>>>> feature/Categoria
    """
    Single-byte encoding language association. Some code page are heavily linked to particular language(s).
    This function does the correspondence.
    """
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    unicode_ranges: list[str] = encoding_unicode_range(iana_name)
    primary_range: str | None = None
=======
    unicode_ranges: List[str] = encoding_unicode_range(iana_name)
    primary_range: Optional[str] = None
>>>>>>> feature/Ingredientes
=======
    unicode_ranges: list[str] = encoding_unicode_range(iana_name)
    primary_range: str | None = None
>>>>>>> feature/Cuenta
=======
    unicode_ranges: list[str] = encoding_unicode_range(iana_name)
    primary_range: str | None = None
>>>>>>> feature/Categoria

    for specified_range in unicode_ranges:
        if "Latin" not in specified_range:
            primary_range = specified_range
            break

    if primary_range is None:
        return ["Latin Based"]

    return unicode_range_languages(primary_range)


@lru_cache()
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
def mb_encoding_languages(iana_name: str) -> list[str]:
=======
def mb_encoding_languages(iana_name: str) -> List[str]:
>>>>>>> feature/Ingredientes
=======
def mb_encoding_languages(iana_name: str) -> list[str]:
>>>>>>> feature/Cuenta
=======
def mb_encoding_languages(iana_name: str) -> list[str]:
>>>>>>> feature/Categoria
    """
    Multi-byte encoding language association. Some code page are heavily linked to particular language(s).
    This function does the correspondence.
    """
    if (
        iana_name.startswith("shift_")
        or iana_name.startswith("iso2022_jp")
        or iana_name.startswith("euc_j")
        or iana_name == "cp932"
    ):
        return ["Japanese"]
    if iana_name.startswith("gb") or iana_name in ZH_NAMES:
        return ["Chinese"]
    if iana_name.startswith("iso2022_kr") or iana_name in KO_NAMES:
        return ["Korean"]

    return []


@lru_cache(maxsize=LANGUAGE_SUPPORTED_COUNT)
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
def get_target_features(language: str) -> tuple[bool, bool]:
=======
def get_target_features(language: str) -> Tuple[bool, bool]:
>>>>>>> feature/Ingredientes
=======
def get_target_features(language: str) -> tuple[bool, bool]:
>>>>>>> feature/Cuenta
=======
def get_target_features(language: str) -> tuple[bool, bool]:
>>>>>>> feature/Categoria
    """
    Determine main aspects from a supported language if it contains accents and if is pure Latin.
    """
    target_have_accents: bool = False
    target_pure_latin: bool = True

    for character in FREQUENCIES[language]:
        if not target_have_accents and is_accentuated(character):
            target_have_accents = True
        if target_pure_latin and is_latin(character) is False:
            target_pure_latin = False

    return target_have_accents, target_pure_latin


def alphabet_languages(
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> feature/Cuenta
=======
>>>>>>> feature/Categoria
    characters: list[str], ignore_non_latin: bool = False
) -> list[str]:
    """
    Return associated languages associated to given characters.
    """
    languages: list[tuple[str, float]] = []
<<<<<<< HEAD
<<<<<<< HEAD
=======
    characters: List[str], ignore_non_latin: bool = False
) -> List[str]:
    """
    Return associated languages associated to given characters.
    """
    languages: List[Tuple[str, float]] = []
>>>>>>> feature/Ingredientes
=======
>>>>>>> feature/Cuenta
=======
>>>>>>> feature/Categoria

    source_have_accents = any(is_accentuated(character) for character in characters)

    for language, language_characters in FREQUENCIES.items():
        target_have_accents, target_pure_latin = get_target_features(language)

        if ignore_non_latin and target_pure_latin is False:
            continue

        if target_have_accents is False and source_have_accents:
            continue

        character_count: int = len(language_characters)

        character_match_count: int = len(
            [c for c in language_characters if c in characters]
        )

        ratio: float = character_match_count / character_count

        if ratio >= 0.2:
            languages.append((language, ratio))

    languages = sorted(languages, key=lambda x: x[1], reverse=True)

    return [compatible_language[0] for compatible_language in languages]


def characters_popularity_compare(
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    language: str, ordered_characters: list[str]
=======
    language: str, ordered_characters: List[str]
>>>>>>> feature/Ingredientes
=======
    language: str, ordered_characters: list[str]
>>>>>>> feature/Cuenta
=======
    language: str, ordered_characters: list[str]
>>>>>>> feature/Categoria
) -> float:
    """
    Determine if a ordered characters list (by occurrence from most appearance to rarest) match a particular language.
    The result is a ratio between 0. (absolutely no correspondence) and 1. (near perfect fit).
    Beware that is function is not strict on the match in order to ease the detection. (Meaning close match is 1.)
    """
    if language not in FREQUENCIES:
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
        raise ValueError(f"{language} not available")
=======
        raise ValueError("{} not available".format(language))
>>>>>>> feature/Ingredientes
=======
        raise ValueError(f"{language} not available")
>>>>>>> feature/Cuenta
=======
        raise ValueError(f"{language} not available")
>>>>>>> feature/Categoria

    character_approved_count: int = 0
    FREQUENCIES_language_set = set(FREQUENCIES[language])

    ordered_characters_count: int = len(ordered_characters)
    target_language_characters_count: int = len(FREQUENCIES[language])

    large_alphabet: bool = target_language_characters_count > 26

    for character, character_rank in zip(
        ordered_characters, range(0, ordered_characters_count)
    ):
        if character not in FREQUENCIES_language_set:
            continue

        character_rank_in_language: int = FREQUENCIES[language].index(character)
        expected_projection_ratio: float = (
            target_language_characters_count / ordered_characters_count
        )
        character_rank_projection: int = int(character_rank * expected_projection_ratio)

        if (
            large_alphabet is False
            and abs(character_rank_projection - character_rank_in_language) > 4
        ):
            continue

        if (
            large_alphabet is True
            and abs(character_rank_projection - character_rank_in_language)
            < target_language_characters_count / 3
        ):
            character_approved_count += 1
            continue

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> feature/Cuenta
=======
>>>>>>> feature/Categoria
        characters_before_source: list[str] = FREQUENCIES[language][
            0:character_rank_in_language
        ]
        characters_after_source: list[str] = FREQUENCIES[language][
            character_rank_in_language:
        ]
        characters_before: list[str] = ordered_characters[0:character_rank]
        characters_after: list[str] = ordered_characters[character_rank:]
<<<<<<< HEAD
<<<<<<< HEAD
=======
        characters_before_source: List[str] = FREQUENCIES[language][
            0:character_rank_in_language
        ]
        characters_after_source: List[str] = FREQUENCIES[language][
            character_rank_in_language:
        ]
        characters_before: List[str] = ordered_characters[0:character_rank]
        characters_after: List[str] = ordered_characters[character_rank:]
>>>>>>> feature/Ingredientes
=======
>>>>>>> feature/Cuenta
=======
>>>>>>> feature/Categoria

        before_match_count: int = len(
            set(characters_before) & set(characters_before_source)
        )

        after_match_count: int = len(
            set(characters_after) & set(characters_after_source)
        )

        if len(characters_before_source) == 0 and before_match_count <= 4:
            character_approved_count += 1
            continue

        if len(characters_after_source) == 0 and after_match_count <= 4:
            character_approved_count += 1
            continue

        if (
            before_match_count / len(characters_before_source) >= 0.4
            or after_match_count / len(characters_after_source) >= 0.4
        ):
            character_approved_count += 1
            continue

    return character_approved_count / len(ordered_characters)


<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
def alpha_unicode_split(decoded_sequence: str) -> list[str]:
=======
def alpha_unicode_split(decoded_sequence: str) -> List[str]:
>>>>>>> feature/Ingredientes
=======
def alpha_unicode_split(decoded_sequence: str) -> list[str]:
>>>>>>> feature/Cuenta
=======
def alpha_unicode_split(decoded_sequence: str) -> list[str]:
>>>>>>> feature/Categoria
    """
    Given a decoded text sequence, return a list of str. Unicode range / alphabet separation.
    Ex. a text containing English/Latin with a bit a Hebrew will return two items in the resulting list;
    One containing the latin letters and the other hebrew.
    """
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    layers: dict[str, str] = {}
=======
    layers: Dict[str, str] = {}
>>>>>>> feature/Ingredientes
=======
    layers: dict[str, str] = {}
>>>>>>> feature/Cuenta
=======
    layers: dict[str, str] = {}
>>>>>>> feature/Categoria

    for character in decoded_sequence:
        if character.isalpha() is False:
            continue

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
        character_range: str | None = unicode_range(character)
=======
        character_range: Optional[str] = unicode_range(character)
>>>>>>> feature/Ingredientes
=======
        character_range: str | None = unicode_range(character)
>>>>>>> feature/Cuenta
=======
        character_range: str | None = unicode_range(character)
>>>>>>> feature/Categoria

        if character_range is None:
            continue

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
        layer_target_range: str | None = None
=======
        layer_target_range: Optional[str] = None
>>>>>>> feature/Ingredientes
=======
        layer_target_range: str | None = None
>>>>>>> feature/Cuenta
=======
        layer_target_range: str | None = None
>>>>>>> feature/Categoria

        for discovered_range in layers:
            if (
                is_suspiciously_successive_range(discovered_range, character_range)
                is False
            ):
                layer_target_range = discovered_range
                break

        if layer_target_range is None:
            layer_target_range = character_range

        if layer_target_range not in layers:
            layers[layer_target_range] = character.lower()
            continue

        layers[layer_target_range] += character.lower()

    return list(layers.values())


<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
def merge_coherence_ratios(results: list[CoherenceMatches]) -> CoherenceMatches:
=======
def merge_coherence_ratios(results: List[CoherenceMatches]) -> CoherenceMatches:
>>>>>>> feature/Ingredientes
=======
def merge_coherence_ratios(results: list[CoherenceMatches]) -> CoherenceMatches:
>>>>>>> feature/Cuenta
=======
def merge_coherence_ratios(results: list[CoherenceMatches]) -> CoherenceMatches:
>>>>>>> feature/Categoria
    """
    This function merge results previously given by the function coherence_ratio.
    The return type is the same as coherence_ratio.
    """
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    per_language_ratios: dict[str, list[float]] = {}
=======
    per_language_ratios: Dict[str, List[float]] = {}
>>>>>>> feature/Ingredientes
=======
    per_language_ratios: dict[str, list[float]] = {}
>>>>>>> feature/Cuenta
=======
    per_language_ratios: dict[str, list[float]] = {}
>>>>>>> feature/Categoria
    for result in results:
        for sub_result in result:
            language, ratio = sub_result
            if language not in per_language_ratios:
                per_language_ratios[language] = [ratio]
                continue
            per_language_ratios[language].append(ratio)

    merge = [
        (
            language,
            round(
                sum(per_language_ratios[language]) / len(per_language_ratios[language]),
                4,
            ),
        )
        for language in per_language_ratios
    ]

    return sorted(merge, key=lambda x: x[1], reverse=True)


def filter_alt_coherence_matches(results: CoherenceMatches) -> CoherenceMatches:
    """
    We shall NOT return "English—" in CoherenceMatches because it is an alternative
    of "English". This function only keeps the best match and remove the em-dash in it.
    """
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    index_results: dict[str, list[float]] = dict()
=======
    index_results: Dict[str, List[float]] = dict()
>>>>>>> feature/Ingredientes
=======
    index_results: dict[str, list[float]] = dict()
>>>>>>> feature/Cuenta
=======
    index_results: dict[str, list[float]] = dict()
>>>>>>> feature/Categoria

    for result in results:
        language, ratio = result
        no_em_name: str = language.replace("—", "")

        if no_em_name not in index_results:
            index_results[no_em_name] = []

        index_results[no_em_name].append(ratio)

    if any(len(index_results[e]) > 1 for e in index_results):
        filtered_results: CoherenceMatches = []

        for language in index_results:
            filtered_results.append((language, max(index_results[language])))

        return filtered_results

    return results


@lru_cache(maxsize=2048)
def coherence_ratio(
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    decoded_sequence: str, threshold: float = 0.1, lg_inclusion: str | None = None
=======
    decoded_sequence: str, threshold: float = 0.1, lg_inclusion: Optional[str] = None
>>>>>>> feature/Ingredientes
=======
    decoded_sequence: str, threshold: float = 0.1, lg_inclusion: str | None = None
>>>>>>> feature/Cuenta
=======
    decoded_sequence: str, threshold: float = 0.1, lg_inclusion: str | None = None
>>>>>>> feature/Categoria
) -> CoherenceMatches:
    """
    Detect ANY language that can be identified in given sequence. The sequence will be analysed by layers.
    A layer = Character extraction by alphabets/ranges.
    """

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    results: list[tuple[str, float]] = []
=======
    results: List[Tuple[str, float]] = []
>>>>>>> feature/Ingredientes
=======
    results: list[tuple[str, float]] = []
>>>>>>> feature/Cuenta
=======
    results: list[tuple[str, float]] = []
>>>>>>> feature/Categoria
    ignore_non_latin: bool = False

    sufficient_match_count: int = 0

    lg_inclusion_list = lg_inclusion.split(",") if lg_inclusion is not None else []
    if "Latin Based" in lg_inclusion_list:
        ignore_non_latin = True
        lg_inclusion_list.remove("Latin Based")

    for layer in alpha_unicode_split(decoded_sequence):
        sequence_frequencies: TypeCounter[str] = Counter(layer)
        most_common = sequence_frequencies.most_common()

        character_count: int = sum(o for c, o in most_common)

        if character_count <= TOO_SMALL_SEQUENCE:
            continue

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
        popular_character_ordered: list[str] = [c for c, o in most_common]
=======
        popular_character_ordered: List[str] = [c for c, o in most_common]
>>>>>>> feature/Ingredientes
=======
        popular_character_ordered: list[str] = [c for c, o in most_common]
>>>>>>> feature/Cuenta
=======
        popular_character_ordered: list[str] = [c for c, o in most_common]
>>>>>>> feature/Categoria

        for language in lg_inclusion_list or alphabet_languages(
            popular_character_ordered, ignore_non_latin
        ):
            ratio: float = characters_popularity_compare(
                language, popular_character_ordered
            )

            if ratio < threshold:
                continue
            elif ratio >= 0.8:
                sufficient_match_count += 1

            results.append((language, round(ratio, 4)))

            if sufficient_match_count >= 3:
                break

    return sorted(
        filter_alt_coherence_matches(results), key=lambda x: x[1], reverse=True
    )
